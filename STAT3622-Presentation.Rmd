---
title: "STAT3622-Final Project-Stroke Prediction"
author: "Gong Kening Nicole"
date: "2023-04-18"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load data and necessary packages
library(ggplot2)
library(dplyr)
df <- read.csv("C:/Users/Nicole Gong/Desktop/STAT3622/Final project/stroke.csv",
               header=TRUE,sep=',',stringsAsFactors=F)
df$stroke = as.factor(df$stroke)
```

# 1.Introduction  
  
## 1.1 Background Information  
  
As more medicine papers examining the risk of stroke have been published and raise the point that prevention is better than cure for stroke. Knowing which factors are more likely to cause strokes, for example, age, or glucose level is crucial. And this is our main topic in this report: Predict whether a patient will have a stroke by the existing variables.  
  
## 1.2 Data Description
```{r}
#view first ten rows of data
head(df,n=10)
#view data type
str(df)
```
Gender is divided into male and female; For hypertension, heart_disease and stroke, 0 means the patient do not have the disease, 1 otherwise; ever_married has two answers yes and no; Residence_type has two answers urban and rural; work_type and smoking_status is classified into 5 and 4 situations respectively. age, avg_glucose_level and bmi is recorded as numerical values. To solve the target problem, set stroke to be the dependent variable, and others to be the independent variables.  
  
# 2. Exploratory Data Analysis  
## 2.1 Data Cleaning  
### 2.1.1 repeated values  
First, we check the duplicated entries in the data set, and then eliminate them from it.  Actually we can find there are no repeated values, so the row number does not change, which is good.  
```{r}
df <- df[!duplicated(df),]
```
  
### 2.1.2 data type conversion  
In order to facilitate the further modelling, id is deleted, because id is actually unique for every patient which is useless for us in prediction.  
  
For gender, we can convert male to 1 and female to 0. And the similar method is applied for marriage situation and residence type.  
  
For work_type and smoking_status, we assign different numerical number to replace the characters of different situations. A point to note that, number 5 is used to replace the ‘unknown’. This is because that the number of 'unknown' is quite large, which counts for around 1544 samples, so we decide to treat it as a new group rather than to change it.  
```{r}
#delete id
df <- df[, -1]
#convert gender to 1 and 0
df <- df[df$gender != 'Other',]
df$gender <- ifelse(df$gender == 'Male',1,0)
#convert ever_marrige: yes to 1 and no to 0.
df$ever_married <- ifelse(df$ever_married == 'Yes',1,0)
#convert work type:
unique(df$work_type)
df$work_type <- ifelse(df$work_type == 'Children',1, 
                       ifelse(df$work_type == 'Govt_job',2, 
                              ifelse(df$work_type == 'Never_worked', 3, 
                                     ifelse(df$work_type == 'Private', 4, 5)) ))
#convert residence_type
df$Residence_type <- ifelse(df$Residence_type == 'Urban',1,0)
#convert smoking status
df$smoking_status <- ifelse(df$smoking_status == 'formerly smoked',1, 
                            ifelse(df$smoking_status == 'never smoked',2, 
                                   ifelse(df$smoking_status == 'smokes', 3, 4) ))
df$gender <- as.factor(df$gender)
df$hypertension <- as.factor(df$hypertension)
df$heart_disease <- as.factor(df$heart_disease)
df$ever_married <- as.factor(df$ever_married)
df$Residence_type <- as.factor(df$Residence_type)
df$stroke <- as.factor(df$stroke)

```
  
### 2.1.3 missing values  
Firstly, is.na function is used to check whether there are missing values in our data set, and we can find that for the variable ‘bmi’, there are 201 missing values, which is around 3.93% of the sample size. This is quite a large percentage. Therefore, in this case, we can not simply drop it, which may enlarge the errors.  
  
To deal with the missing values, we then plot a density plot to see the distribution of bmi. From the graph, it shows that the distribution is approximately normal and slightly right-skewed, so it is reasonable to use the median value to replace the missing values.  
```{r}
#using median value to replace missing values
df$bmi <- ifelse(df$bmi == 'N/A', NA, df$bmi)
col_mv <- apply(is.na(df), 2, sum)
df$bmi <- as.numeric(df$bmi)
p <- ggplot(data = df, aes(x=bmi))+
  geom_density(fill='lightblue', alpha = 0.4) +
  xlab("bmi")+
  ylab("Density")+
  ggtitle("Density plot of bmi")
df$bmi[is.na(df$bmi)] <- median(df$bmi, na.rm = TRUE)
p
```  
  
### 2.1.4 outliers  
There are many outliers of the average glucose level shown in the box plot. However, after drawing another density plot, although the number of outliers seems quite high in the data, those high average glucose level areas actually also show a trend and conclude some characteristics for the stroke and non-stroke groups, so finally we decided to keep those extreme values.   
```{r}
boxplot(df$avg_glucose_level)
ggplot(df, aes(x=avg_glucose_level,col=stroke))+
  geom_density()
```
  
## 2.2 features & visualization  
### 2.2.1 Numerical variables  
```{r}
#age vs stroke
p1 <- ggplot(df, aes(age,fill=stroke)) +
  geom_density(alpha=0.5)
p1
```
  
From the density graph, we can find that for those patient without stroke, the distribution has no obvious trend, but for those who got stroke, see, the blue area here, from the line we can find that their ages mainly concentrated on 60 to 80 years and those who below 40 years old seldom get stroke.   
  
```{r}
#glucose vs stroke
p2 <- ggplot(df, aes(avg_glucose_level,fill=stroke)) +
  geom_density(alpha=0.5)
p2
```
  
It is obvious that those patients without stroke is more likely to distributed at 90 and scatter around, while those stroke population’s glucose levels are more likely to distributed within a relatively stable interval, from 100 to 200.  
  
### 2.2.2 Categorical variables  
```{r}
#gender vs stroke
t1 <- table(df$gender,df$stroke)
ggplot(df, aes(x=gender,fill=stroke))+
  geom_bar(width=0.5)+
  scale_fill_manual(values=c("grey","light yellow"))

rate_for_female <- t1[1,2]/(t1[1,1]+t1[1,2])
rate_for_male <- t1[2,2]/(t1[2,1]+t1[2,2])
```
  
For gender, more women were surveyed, but in terms of whether or not they had a stroke, actually two gender have similar percentages for 0.047 and 0.051, it is slightly higher for male. 
  
```{r}
#hypertension vs stroke
t2 <- table(df$hypertension,df$stroke)
ggplot(df, aes(x=hypertension,fill=stroke))+
  geom_bar(width=0.5)+
  scale_fill_manual(values=c("grey","light yellow"))

hypertension0 <- t2[1,2]/(t2[1,1]+t2[1,2])
hypertension1 <- t2[2,2]/(t2[2,1]+t2[2,2])
```
  
Obviously, rate for hypertension patient to get stroke is higher, impling relationship.  
  
```{r}
#heart disease vs stroke
t3 <- table(df$heart_disease,df$stroke)
ggplot(df, aes(x=heart_disease,fill=stroke))+
  geom_bar(width=0.5)+
  scale_fill_manual(values=c("grey","light yellow"))

heart0 <- t3[1,2]/(t3[1,1]+t3[1,2])
heart1 <- t3[2,2]/(t3[2,1]+t3[2,2])

```

For the heart disease, the significant difference in percentage also implies that there is a relationship between heart disease and stroke.   

more similar bar plots are shown in this shiny link here:  
https://a45kbf-zhuoya-qin.shinyapps.io/Rdocs/?_ga=2.263771012.901240130.1682334379-823904070.1682334379
  
Because RMD cannot output the interactive panel, we just show the code in the Appendix of this report for references.  
  
### 2.2.3 Correlation
To better show the relationship within various variables, we also use correlation matrix to see whether their are some variables with strong correlation with others.    
```{r}
#correlation matrix
library(ggcorrplot)
df.corr = df
df.corr$gender <- as.numeric(df.corr$gender)
df.corr$hypertension <- as.numeric(df.corr$hypertension)
df.corr$heart_disease <- as.numeric(df.corr$heart_disease)
df.corr$ever_married <- as.numeric(df.corr$ever_married)
df.corr$Residence_type <- as.numeric(df.corr$Residence_type)
df.corr$stroke <- as.numeric(df.corr$stroke)
cor_matrix <- cor(df.corr)
ggcorrplot(cor_matrix, type="lower", lab=T, lab_size=3,
           outline.color = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("orange","white","light blue"))
```
  
From the matrix, we can find that age is a relatively more significant factor as it correlates with correlation of value 0.25. However, we can find that actually most of the variables have very low correlation coefficients with stroke.  
  
This tells us that the data cannot be concluded by a linear relationship. In this next section, we will try different models to fit the data and get the predictions.     
  
# 3. Modelling  
## 3.1 Data Pre-processing
Since the data set only contains 3 numerical variables, and there is no quite significant difference of their scales, which means that all the variables could contribute equally to the models. Therefore, in order to store more characteristics of the original values, standardization is not conducted in this case.  
  
### 3.1.1 Split Data   
In order to build a reasonable model to conduct the classification, 75% entries from the original data is separated to the training set to construct models, and the remaining is separated to the testing set to test the model accuracy for the out-of-sample situations.  
```{r}
df$work_type = as.factor(df$work_type)
df$smoking_status = as.factor(df$smoking_status)
# 0.75 training + 0.25 testing
library(caret)
set.seed(7)
splitindex = createDataPartition(df$stroke, p=0.75, list=FALSE)
train = df[splitindex,] # 70% of data to training
test = df[-splitindex,] # remaining 30% for test
```
  
### 3.1.2 Imbalanced Data
Because the data of stroke = 0 and stroke = 1 is imbalanced, which will make the algorithms to produce high bias on the predictions. The balancing process is conducted, keeping the total number of observations the same but simulate and make the proportions of each group close.    
```{r}
library(ROSE)
train.rose = ROSE(stroke~., data = train, seed = 7)$data
table(train.rose$stroke)
# Visualization of balancing
par(mfrow=c(1,2))
plot(train$stroke, xlab="Stroke (0/1)", ylab="Count", 
     main = "Imbalanced Distribution By Stroke",
     col = c("#156077","#f46f20"))

plot(train.rose$stroke, xlab="Stroke (0/1)", ylab="Count", 
     main = "Balanced Data By Stroke",
     col = c("#156077","#f46f20"))
```  
  
Currently, the train set has close number of the patients with and without stroke, which means that the train data set is balanced.  
  
## 3.2 Model Constructing  
### 3.2.1 Logistic Regression  
The logistic regression could classify the binary response variable stroke effectively. The model is first built fully with all the independent variables, then through step-wise variable selection method, the model is further simplified.      
```{r}
library(MASS)
library(car)
full.glm = glm(stroke~., family="binomial", data=train.rose)
# construct the reduced model by stepwise method
fit.glm = full.glm %>% stepAIC(trace = FALSE)
summary(fit.glm)
vif(fit.glm)
```  
  
Most of the positive coefficients there make sense with the descriptive statistics. To be more specific, from the EDA and model analysis, age, hypertension, heart_disease and average glucose level is positively related with the probability of stroke can be statistically concluded.  Also, the small vif values indicate that there is no multicollinearity in the reduced model.  
  
With the logistic regression model, setting the threshold to be 0.5 and do the prediction for the testing set. The confusion matrix looks like the following.  
```{r}
# Accuracy on testing set
PredictTest = predict(fit.glm, test, type="response")
table(test$stroke, PredictTest > 0.5)
# Visualization of confusion matrix
glm.cmatrix = as.table(matrix(c(918,297,16,46), nrow = 2, byrow = TRUE))
fourfoldplot(glm.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "Logistic Regression Test Confusion Matrix")
```  
  
Therefore, from the confusion matrix, the overall accuracy of the logistic regression model of the the testing set is 75.489%.  
  
### 3.2.2 KNN  
KNN algorithm can also be used in classification problems. In this case, we used the repeated k-fold cross validation method to choose the parameter k which can maximize the accuracy for the training data.  
```{r}
# KNN model
set.seed(7)
trainControl = trainControl(method="cv", number=10)
grid = expand.grid(.k=seq(1,20,by=1))
fit.knn = train(stroke~., data=train.rose, method="knn", 
                 tuneGrid=grid, metric = "Accuracy", trControl=trainControl)
k = fit.knn$bestTune
plot(fit.knn, xlab="k value", ylab="Accuracy",main="Accuracy vs K in KNN")
```
  
According to the result, we can find a trend that with the increase of k-value, the accuracy peaks at k =16, and then becomes stable arpund 76%. For the training data set, k value is chosen to be 16, which makes the accuracy on the training set is the maximum in the searching range of k.   
```{r}
# Accuracy for the test set
library(class)
predict.knn.test = knn(train=train.rose, test=test, cl=train.rose$stroke, k=k)
table(predict.knn.test, test$stroke)
# Visualization of confusion matrix 
knn.cmatrix = as.table(matrix(c(896,16,319,46), nrow = 2, byrow = TRUE))
fourfoldplot(knn.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, main = "KNN Test Confusion Matrix")
```  
The overall accuracy for the KNN model for the test set is now 73.375%. However, although the accuracy is OK in this case, but the false negative rate is very high and this model has a very bad performance on predicting for the patients who actually has a stroke. This makes the model not so feasible in the real-life application. Therefore, KNN model is not so good in this case.   
   
### 3.2.3 Naïve Bayes  
Because the training set has many dummy variables, Naïve Bayes can deal with such situation. To avoid the situation that the algorithm gives a 0 result, we can use Laplacian smoothing to add 1 case for each entry.   
```{r}
set.seed(7)
library(e1071)
fit.nbayes = naiveBayes(y=train.rose$stroke, x=train.rose[,-11], data=train.rose, Laplace = 1)
```
Then use the model to predict for the test set.  
```{r}
# Accuracy on Testing set
bayes.PredictTest = predict(fit.nbayes, newdata = test, type = "class")
table(test$stroke, bayes.PredictTest)
# Confusion matrix on testing set
nb.cmatrix = as.table(matrix(c(863,352,16,46), nrow = 2, byrow = TRUE))
fourfoldplot(nb.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "Naive Bayes Test Confusion Matrix")
```
According to the confusion matrix, the accuracy of Naïve Bayes model for the test set is 71.182%.  
  
### 3.2.4 Support Machine Vector  
SVM is also a strong algorithm to classify the binary groups. We use the linear kernel to train the SVM model. And we get the overall accuracy of 74.001%.  
```{r}
# train the model
set.seed(7)
fit.svm = svm(stroke ~ ., data=train.rose, kernel = "linear")
# Predictions
table(test[,11], predict(fit.svm, newdata = test))
# Confusion matrix
svm.cmatrix = as.table(matrix(c(898,317,15,47), nrow = 2, byrow = TRUE))
fourfoldplot(svm.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "SVM Test Confusion Matrix")
```
  
### 3.2.5 Random Forest    
For the random forest algoritm, we can see the error rate decreases and then becomes stable as the increase of ntree. And we choose the parameter ntree from this stable range. Random Forest gives us a high accuracy at 78.94%.   
```{r}
# train the model
library(randomForest)
set.seed(7)
classifier_RF_b = randomForest(stroke~.,data=train.rose, ntree=500)
plot(classifier_RF_b)
```
```{r}
confusion_mtx_RF <- table(test[,11], predict(classifier_RF_b, newdata = test,type="class"))
confusion_mtx_RF
rf.cmatrix = as.table(matrix(c(976,239,30,32), nrow = 2, byrow = TRUE))
fourfoldplot(rf.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "Random Forest Test Confusion Matrix")
```
  
### 3.2.6 XG Boost and CatBoost   
Among many boosting algorithms, we choose XG Boosting and CatBoosting fro this prediction. The reason is that XGB is very widely used and Catboosting can support the data set with more categorical variables, which is just what we have in our training set.  
```{r}
set.seed(7)
library(xgboost)
# define predictor and response variables in training set
train_x = data.matrix(train.rose[, -11])
train_y = train.rose[,11]
train_y = as.numeric(train_y)-1
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
# define predictor and response variables in testing set
test_x = data.matrix(test[, -11])
test_y = test[,11]
test_y = as.numeric(test_y)-1
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

fit.xgb <- xgboost(data = xgb_train, nround=20, 
                   objective = "binary:logistic", verbose = 0)
```
```{r}
# Predictions 
TestPredict.xgb = predict(fit.xgb, xgb_test)
table(test_y, TestPredict.xgb > 0.5)
xgb.cmatrix = as.table(matrix(c(941,274,18,44), nrow = 2, byrow = TRUE))
fourfoldplot(xgb.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "XGB Test Confusion Matrix")
```
After trying for different trials with the different value of nround, the highest accuracy of XGB of the test set remains at the level of 77.134%.  

```{r}
set.seed(7)
library(catboost)
# Define train for catBoost
train_pool = catboost.load_pool(data = train.rose[,-11], 
                                label = as.numeric(train.rose[,11])-1)
# Define test for catBoost
test_pool = catboost.load_pool(data = test[,-11], 
                                label = as.numeric(test[,11])-1)
# Build Model
fit.catb = catboost.train(learn_pool =train_pool, 
                          params = list(loss_function = 'Logloss',
                                        iterations = 100,
                                        verbose = 0,
                                        od_type = 'Iter',
                                        random_seed=7))
# Prediction
testpredict.catb = catboost.predict(fit.catb, test_pool, 
                                    prediction_type = "Class")
table(as.numeric(test[,11])-1, testpredict.catb > 0.5)
cgb.cmatrix = as.table(matrix(c(932,283,17,45), nrow = 2, byrow = TRUE))
fourfoldplot(cgb.cmatrix, color = c("#156077","#f46f20"),
             conf.level = 0, margin = 1, 
             main = "CatBoost Test Confusion Matrix")
```
The accuracy of the CatBoost model is 76.507%.  
  
# 4. Model Evaluation and Limitation  
## 4.1 Model Evaluation  
Comparing the accuracy of 7 models, XGB and random forest have the best performances. To be more specific, XGB has an overall accuracy of 77.134% and random forest has an accuracy of 78.935%. These two models will be focused on in the following.  
```{r}
acc_compare = data.frame(method = c("Logistic Regression", "KNN", "Naive Bayes",
                                    "SVM", "Random Forest", "XGB", "CatBoost"),
                          accuracy = c(75.489,73.375,71.182,74.001,78.935,77.134,76.507))
acc_compare <- acc_compare[order(-acc_compare$accuracy),]
library(knitr)
kable(acc_compare, format = "markdown")
```
  
### 4.1.1 Feature Importance  
```{r}
# XGB
importance.xgb = xgb.importance(feature_names = colnames(train.rose[,-11]), model = fit.xgb)
ggplot(data=importance.xgb[1:5],mapping=aes(y=Gain,x=Feature, fill=Feature))+
  geom_col()+
  ggtitle("XGB Top5 Feature Importance")
# Random Forest
varImpPlot(classifier_RF_b)
```
We plot the feature importance for each method, and two models give us similar results. To be more specific, age is much more important than other variables, average glucose level and bmi ranked 2 and 3 respectively here. The result here follows the EDA part, where the density plot of age and glucose level performs very differently for stroke=0 and stroke=1, which indicates that they are possible variables contributed a lot to the result.    
  
### 4.1.2 Model Selection between XGB and Random Forest  
To solve the target classification problem, the selection between the two models cannot only depend on its overall accuracy. If we just simply compare their accuracy, Random Forest (78.94%) is slightly better than XGB (77.134%), but the false negative rate of random Forest (48.387%) is much higher than XGB (29.032%). The big false negative rate may wrongly predict for the patients who actually with a stroke. If the model is finally applied in the real life, such wrong prediction will enhance the risks of patients and let them miss the opportunities of diagnosis, which is not what we want. This is what the problems at. Therefore, the final selected model is XG Boosting Classifier.  

## 4.2 Limitations
In the modelling section, the test set used is imbalanced, and the model is built by balanced train set. This operation will decrease the overall accuracy a lot. In our several trials for each model, the overall accuracy with the imbalanced set could reach over 90%, which is about 20% higher than the current model which is built with the balanced data.  
  
But actually, this contains too much bias on predicting more stroke=0. While the balanced model has low overall accuracy but more reasonable class error.  

The balancing process causes the modifications and simulation to the entries of train set, resulting in the difference in accuracy of the train and test sets. To be more specific, the overall accuracy for the train set reaches 85.204% as shown in the next chunk, while the accuracy for the test set is now 77.134%. There is a weaker performance on test set, which means that the model might over-fit slightly for the train set due to the different data set distribution by stroke.  
  
Further improvement might be made by collecting a more balances data set from the real world and modelling again to find a more applicable result.  
```{r}
# XGB Accuracy For Train set  
TrainPredict.xgb = predict(fit.xgb, xgb_train)
table(train_y, TrainPredict.xgb > 0.5)
```
  
# 5. Conclusion  
To conclude, from the machine learning method results, extreme gradient boosting gives the best performance considering not only the accuracy, but also the false negative rate, which is more critical in real world prediction.  
  
And another important point for us to notice is the imbalance of dataset. If we do not address this problem and only pursue the “high accuracy for test set”, then the model will make incorrect prediction, and the process of balancing can help us reach a more valid model.  
  
# 6. Appendix  
Shiny code for reference  
```{r, eval=FALSE}
library(shiny)
library(shinythemes)
```
  
```{r, eval = FALSE}
#define UI for application that draws a bar plot
ui <- fluidPage(
  
  #theme
  theme = shinytheme("slate"),
  #Application title
  titlePanel("Exploratory Data Analysis for Stroke Data"),
  
  #Bar plot
  sidebarLayout(
    sidebarPanel(
      selectInput(inputId = 'var',
                  label = 'variables:',
                  choices = c("gender","age","hypertension","heart_disease","ever_married","work_type",
                              "Residence_type","avg_glucose_level","bmi","smoking_status"),
                  selected = "stroke")
    ),
  
  #main panel    
    mainPanel(
        plotOutput(outputId = "bar")
    )
  )
)

server <- function(input, output){
  output$bar <- renderPlot({
    var <- sym(input$var)
    ggplot(df, aes(x = !!var, fill=stroke))+
        geom_bar(width=0.5)+
        scale_fill_manual(values=c("grey","light yellow"))
  })
}


shinyApp(ui = ui, server = server)
```




